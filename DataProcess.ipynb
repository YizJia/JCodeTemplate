{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file, I will focus on the way the data is handled.\n",
    "\n",
    "The writing of this document was inspired by the mathmatical modeling competition that ended a while ago. The raw data is stored in a variety of formats, and templating the code for reading and processing the data helps me categorize the methods.\n",
    "\n",
    "This document will use Python. Using other coding languages (e.g. matlab) will be given in other documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Read data from txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data in txt files in batches.\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None) #show all columns\n",
    "pd.set_option('display.max_columns', 100) #show 100 lines\n",
    "pd.set_option('display.width', 5000) #Set the display width to avoid text wrapping \n",
    "\n",
    "def read_txtdata_batches(dir_name, data_title):\n",
    "    \"\"\"Read data from txt files in batch\"\"\"\n",
    "\n",
    "    # set file's path\n",
    "    path = \"E:\\\\ProgramNew\\\\Pfile_jupyter\\\\Module21\\\\UBW_data\"\n",
    "    datapath = os.path.join(path, dir_name, \"\")\n",
    "    # print(datapath)\n",
    "\n",
    "    files = os.listdir(datapath)    # Get all names in this folder\n",
    "    print(\"Total file number: \" + str(len(files)))\n",
    "    txts = []\n",
    "    txts.append(data_title)\n",
    "    # i = 0\n",
    "\n",
    "    for file in files:\n",
    "        position = os.path.join(datapath, file)  # Construct an absolute path\n",
    "        # Get the index for the file\n",
    "        file_index = file.split('.')[0]\n",
    "\n",
    "        # Read file\n",
    "        with open(position, \"r\", encoding='UTF-8') as f:\n",
    "            lines = f.readlines()\n",
    "            # if i == 0:\n",
    "            #     line_title = lines[0]\n",
    "            #     line_title = line_title.split(':')\n",
    "            #     line_title.append('FileIndex')\n",
    "            #     txts.append(line_title)\n",
    "\n",
    "            # \n",
    "            for line in lines[1:]:\n",
    "                line=line.strip('\\n')\n",
    "                line_split = line.split(':')\n",
    "                line_split.append(file_index)\n",
    "                txts.append(line_split)\n",
    "            # i = i + 1\n",
    "            f.close()\n",
    "\n",
    "    return txts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Write data to cvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a CSV file\n",
    "import csv\n",
    "import codecs\n",
    "\n",
    "def data_write_csv(file_name, datas):\n",
    "#     file_csv = codecs.open(file_name, 'w', 'utf-8')\n",
    "#     writer = csv.writer(file_csv, delimiter=' ', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "#     for data in datas:\n",
    "#         writer.writerow(data)\n",
    "#     print(\"File saved, processing end!\")\n",
    "\n",
    "    f = open(file_name, 'w', encoding='utf-8', newline='')\n",
    "    writer = csv.writer(f)\n",
    "    for data in datas:\n",
    "        writer.writerow(data)\n",
    "    f.close\n",
    "    print(\"File saved, processing end!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example that using the function read_txtdata_batches and data_write_csv:\n",
    "data_txt = []\n",
    "original_data_title = [\"T\", \"Time\", \"RangeReport\", \"TagID\", \"AnchorID\", \"Dis\", \"DisCheck\", \"DataSerialNum\", \"DataNum\", \"FileIndex\"]\n",
    "data_txt = read_txtdata_batches(\"正常数据\", original_data_title)\n",
    "print(len(data_txt))\n",
    "\n",
    "data_write_csv('normal.csv', data_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据的格数如下图：\n",
    "\n",
    "T:090531087:DecaRangeRTLS:LogFile:\u0001z\u0002?m\u0001?\u0001?\u0001?\u0001?:Conf:Tag0:1:Chan2\n",
    "\n",
    "T:090531088:RR:0:0:760:760:229:3301\n",
    "\n",
    "T:090531088:RR:0:1:4550:4550:229:3301\n",
    "\n",
    "T:090531088:RR:0:2:4550:4550:229:3301\n",
    "\n",
    "T:090531088:RR:0:3:6300:6300:229:3301\n",
    "\n",
    "T:090531296:RR:0:0:760:760:230:3302\n",
    "\n",
    "T:090531296:RR:0:1:4550:4550:230:3302\n",
    "\n",
    "T:090531296:RR:0:2:4550:4550:230:3302\n",
    "\n",
    "T:090531296:RR:0:3:6300:6300:230:3302\n",
    "\n",
    "T:090531513:RR:0:0:770:770:231:3303\n",
    "\n",
    "T:090531513:RR:0:1:4550:4550:231:3303\n",
    "\n",
    "T:090531513:RR:0:2:4550:4550:231:3303\n",
    "\n",
    "T:090531513:RR:0:3:6300:6300:231:3303\n",
    "\n",
    "T:090531711:RR:0:0:780:780:232:3304\n",
    "\n",
    "T:090531711:RR:0:1:4550:4550:232:3304\n",
    "\n",
    "T:090531712:RR:0:2:4550:4550:232:3304"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Show data information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Use DataFrame object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will use some usefule methods in DataFrame object to show data information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I mainly use methods of DataFrame object such as:\n",
    "\n",
    "1. DataFrame.head()\n",
    "\n",
    "2. DataFrame.shape\n",
    "\n",
    "3. DataFrame.describe()\n",
    "\n",
    "4. DataFrame.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data_txt = pd.DataFrame(data_txt[1:], columns=data_txt[0])\n",
    "print(\"Head entries of data:\")\n",
    "print(pd_data_txt.head())\n",
    "print(60 * '#')\n",
    "print(\"The shape of data:\")\n",
    "print(pd_data_txt.shape)\n",
    "print(60 * '#')\n",
    "print(\"The describe of data:\")\n",
    "print(pd_data_txt.describe())\n",
    "print(60 * '#')\n",
    "print(\"Information for data:\")\n",
    "print(pd_data_txt.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct table to show unique and missing information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note：** \n",
    "1. nunique值为1是不具备任何意义的，各种值都一样，不存在区分性，应当删除\n",
    "2. 变量缺失值很多，如达到95%以上，亦可以考虑删除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show something useful information of data\n",
    "def show_unique_miss(mydata):\n",
    "    stats = []\n",
    "    for col in mydata.columns:\n",
    "        stats.append((col, mydata[col].nunique(),\n",
    "                     mydata[col].isnull().sum() * 100/mydata.shape[0], \n",
    "                     mydata[col].value_counts(normalize=True, dropna=False).values[0] * 100,\n",
    "                     mydata[col].dtype))\n",
    "        stats_df = pd.DataFrame(stats, columns=['Feature', 'Unique_values', 'Percentage of missing values', \n",
    "                                               'Percentage of values in the biggest category', 'type'])\n",
    "\n",
    "#  Histogram of missing values of variables.\n",
    "# Histogram of missing values of variables.\n",
    "    # plot the missing infromation\n",
    "    # missing = mydata.isnull().sum()\n",
    "    # missing = missing[missing > 0]\n",
    "    # missing.sort_values(inplace=True)\n",
    "    # missing.plot.bar()\n",
    "    \n",
    "    return stats_df.sort_values('Percentage of missing values', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Data Miner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For continuous univariate visualization view the distribution of the observed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def show_distribution_data(pd_data):\n",
    "    pd_data = pd_data.apply(pd.to_numeric, errors='ignore')\n",
    "    # print(pd_data.info())\n",
    "    df_num = pd_data.select_dtypes(include=['float64', 'int64'])\n",
    "    df_num = df_num[df_num.columns.tolist()[1:9]]\n",
    "    df_num.hist(figsize=(16,20), bins=50, xlabelsize=8,ylabelsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature correlation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mport seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "def show_correlation(pd_data):\n",
    "    corrmat = pd_data.corr()\n",
    "    f, ax = plt.subplots(figsize=(20,9))\n",
    "    sns.heatmap(corrmat, vmax=0.8, square=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "739b7fde0239f970fe471183831ce779383fd06bd8b94b3de65ce716bb351d82"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
